{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fecf24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from meds_pipeline import Tokenizer\n",
    "import pickle\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from stream_reconstructor import StreamReconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fc0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = pl.scan_parquet('./data/raw/train/0.parquet')\n",
    "df = lf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7b68d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 | 400552.18 rps\n",
      "200000 | 404080.36 rps\n",
      "300000 | 395221.77 rps\n",
      "400000 | 390312.24 rps\n",
      "500000 | 402084.14 rps\n",
      "600000 | 397384.34 rps\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/roberts/project/pi_ajl89/kam385/bulk_tokenizer/.venv/lib/python3.13/site-packages/meds_pipeline/tokenizers/tokenizer.py:158\u001b[39m, in \u001b[36mTokenizer.encode\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    156\u001b[39m     last_time = row[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# add the tokens for the actual data row. if the first token is the class we're on, skip it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m n_ids,n_vals = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_ids[\u001b[32m0\u001b[39m] == last_class:\n\u001b[32m    160\u001b[39m     ids.extend(n_ids[\u001b[32m1\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/roberts/project/pi_ajl89/kam385/bulk_tokenizer/.venv/lib/python3.13/site-packages/meds_pipeline/tokenizers/tokenizer.py:296\u001b[39m, in \u001b[36mTokenizer.tokenize_row\u001b[39m\u001b[34m(self, row, row_idx)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_type == \u001b[33m'\u001b[39m\u001b[33mcontinuous\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# vals = np.full(len(ids)+1, np.nan)\u001b[39;00m\n\u001b[32m    295\u001b[39m     vals = [np.nan] * (\u001b[38;5;28mlen\u001b[39m(ids) + \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     vals[-\u001b[32m1\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumeric_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     ids.append(\u001b[38;5;28mself\u001b[39m.num_token)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# t = \"Q\"+str(self.digitize(code,numeric_value))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/roberts/project/pi_ajl89/kam385/bulk_tokenizer/.venv/lib/python3.13/site-packages/meds_pipeline/tokenizers/tokenizer.py:416\u001b[39m, in \u001b[36mTokenizer.scale\u001b[39m\u001b[34m(self, code, x)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m formulas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformulas\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/roberts/project/pi_ajl89/kam385/bulk_tokenizer/.venv/lib/python3.13/site-packages/meds_pipeline/tokenizers/tokenizer.py:41\u001b[39m, in \u001b[36m_scaling_formulas.<locals>.<lambda>\u001b[39m\u001b[34m(p, x)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_scaling_formulas\u001b[39m():\n\u001b[32m     34\u001b[39m     eps = \u001b[32m1e-8\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     36\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnormal\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     37\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: (x - p[\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m]) / math.sqrt(\u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33mvar\u001b[39m\u001b[33m\"\u001b[39m], eps)),\n\u001b[32m     38\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33munscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: x * math.sqrt(\u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33mvar\u001b[39m\u001b[33m\"\u001b[39m], eps)) + p[\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     39\u001b[39m         },\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlognormal\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: (\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m - p[\u001b[33m\"\u001b[39m\u001b[33mmu\u001b[39m\u001b[33m\"\u001b[39m]) / math.sqrt(\u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33msigma2\u001b[39m\u001b[33m\"\u001b[39m], eps)),\n\u001b[32m     42\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33munscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: math.exp(x * math.sqrt(\u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33msigma2\u001b[39m\u001b[33m\"\u001b[39m], eps)) + p[\u001b[33m\"\u001b[39m\u001b[33mmu\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     43\u001b[39m         },\n\u001b[32m     44\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgamma\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     45\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: (x - p[\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m] * p[\u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m]) / math.sqrt(\u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m] * p[\u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m]**\u001b[32m2\u001b[39m, eps)),\n\u001b[32m     46\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33munscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: x * math.sqrt(\u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m] * p[\u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m]**\u001b[32m2\u001b[39m, eps)) + p[\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m] * p[\u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     47\u001b[39m         },\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mminmax\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     49\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: (x - p[\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m]) / \u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m] - p[\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m], eps),\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33munscale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p, x: x * \u001b[38;5;28mmax\u001b[39m(p[\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m] - p[\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m], eps) + p[\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     51\u001b[39m         }\n\u001b[32m     52\u001b[39m     }\n",
      "\u001b[31mValueError\u001b[39m: math domain error"
     ]
    }
   ],
   "source": [
    "enc = tok.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading tokenizer\n",
    "tok_path = '/nfs/roberts/project/pi_ajl89/kam385/mimic-iv/v1.1/tokenized/bpe_cont_factored_4k/tok.pkl'\n",
    "with open(tok_path, \"rb\") as f:\n",
    "    tok = pickle.load(f)\n",
    "\n",
    "tok\n",
    "# tok.ttoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20224d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edges': [1.0,\n",
       "  60.0,\n",
       "  360.0,\n",
       "  900.0,\n",
       "  1979.0,\n",
       "  3540.0,\n",
       "  6120.0,\n",
       "  17760.0,\n",
       "  2903205240.0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show bins, if wanted\n",
    "tok.bin_params['<TIME>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301cf897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([38631, 11437, 21810, ..., 38636, 14058, 19392],\n",
       "       shape=(93849910,), dtype=uint16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading binary of tokenized event sequence\n",
    "bin_path = '/nfs/roberts/project/pi_ajl89/kam385/mimic-iv/v1/tokenized/concept_discrete_factored/test.bin'\n",
    "data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcfc039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 38631 -> Code: <sos> \n",
      "Token: 11437 -> Code: <GENDER> \n",
      "Token: 21810 -> Code: M \n",
      "Token: 15992 -> Code: <MEDS_BIRTH> \n",
      "Token: 38642 -> Code: <TIME> \n",
      "Token: 38640 -> Code: Q8 \n",
      "Token: 2396 -> Code: <TRANSFER_TO> \n",
      "Token: 5533 -> Code: ED \n",
      "Token: 8537 -> Code: Emergency Department \n",
      "Token: 38631 -> Code: <sos> \n",
      "Token: 11437 -> Code: <GENDER> \n",
      "Token: 36186 -> Code: F \n",
      "Token: 15992 -> Code: <MEDS_BIRTH> \n",
      "Token: 38642 -> Code: <TIME> \n",
      "Token: 38640 -> Code: Q8 \n",
      "Token: 20034 -> Code: <LAB> \n",
      "Token: 14433 -> Code: Blood Pressure Systolic \n",
      "Token: 38639 -> Code: Q7 \n",
      "Token: 2859 -> Code: Blood Pressure Diastolic \n",
      "Token: 38639 -> Code: Q7 \n"
     ]
    }
   ],
   "source": [
    "for i in data[:20]:\n",
    "    print(f'Token: {i} -> Code: {tok.itot[i]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ace3c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = StreamReconstructor(tokenizer_vocab=tok.ttoi, bins=tok.bin_params) #if you were using a continuous tokenization method, you would have to pass scaling_params=tok.scaling_params instead of bins=tok.bin_params\n",
    "t, x, v = sr.reconstruct(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27ccd7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170937000000, 170937000000, 170937000000, 170937000000, 170937000000, 170937000000, 170937000000, 170937000000, 170937000000, 170937000000]\n",
      "[18996, 38633, 25672, 38639, 28081, 38635, 10655, 38636, 14479, 38637]\n"
     ]
    }
   ],
   "source": [
    "print(t[200:210])\n",
    "print(x[200:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b560100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 11437 -> Code: <GENDER> \n",
      "Token: 21810 -> Code: M \n",
      "Token: 15992 -> Code: <MEDS_BIRTH> \n",
      "Token: 20034 -> Code: <LAB> \n",
      "Token: 14433 -> Code: Blood Pressure Systolic \n",
      "Token: 38638 -> Code: Q6 \n",
      "Token: 2859 -> Code: Blood Pressure Diastolic \n",
      "Token: 38639 -> Code: Q7 \n",
      "Token: 12582 -> Code: <PROCEDURE> \n",
      "Token: 23306 -> Code: ICD \n",
      "Token: 17867 -> Code: 10 \n",
      "Token: 36551 -> Code: 0DV44CZ \n",
      "Token: 2396 -> Code: <TRANSFER_TO> \n",
      "Token: 37137 -> Code: admit \n",
      "Token: 16444 -> Code: Discharge Lounge \n",
      "Token: 33548 -> Code: <HOSPITAL_ADMISSION> \n",
      "Token: 21061 -> Code: SURGICAL SAME DAY ADMISSION \n",
      "Token: 23427 -> Code: PHYSICIAN REFERRAL \n",
      "Token: 2396 -> Code: <TRANSFER_TO> \n",
      "Token: 16438 -> Code: transfer \n"
     ]
    }
   ],
   "source": [
    "for i in x[:20]:\n",
    "    print(f'Token: {i} -> Code: {tok.itot[i]} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
